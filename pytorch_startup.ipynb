{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10\n",
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], \n",
    "[1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])\n",
    "# Create a model\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(n_h, n_out),\n",
    "   nn.Sigmoid())\n",
    "# Construct the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "# Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.25177857279777527\n",
      "epoch:  1  loss:  0.2514360249042511\n",
      "epoch:  2  loss:  0.2510942220687866\n",
      "epoch:  3  loss:  0.25075313448905945\n",
      "epoch:  4  loss:  0.2504127323627472\n",
      "epoch:  5  loss:  0.25007307529449463\n",
      "epoch:  6  loss:  0.24973413348197937\n",
      "epoch:  7  loss:  0.24939589202404022\n",
      "epoch:  8  loss:  0.24905835092067719\n",
      "epoch:  9  loss:  0.24872155487537384\n",
      "epoch:  10  loss:  0.24838541448116302\n",
      "epoch:  11  loss:  0.24804998934268951\n",
      "epoch:  12  loss:  0.2477157860994339\n",
      "epoch:  13  loss:  0.24739190936088562\n",
      "epoch:  14  loss:  0.24706850945949554\n",
      "epoch:  15  loss:  0.24674533307552338\n",
      "epoch:  16  loss:  0.24642284214496613\n",
      "epoch:  17  loss:  0.24610111117362976\n",
      "epoch:  18  loss:  0.24578005075454712\n",
      "epoch:  19  loss:  0.24545972049236298\n",
      "epoch:  20  loss:  0.24514012038707733\n",
      "epoch:  21  loss:  0.2448211908340454\n",
      "epoch:  22  loss:  0.244503453373909\n",
      "epoch:  23  loss:  0.24418681859970093\n",
      "epoch:  24  loss:  0.24387004971504211\n",
      "epoch:  25  loss:  0.24355393648147583\n",
      "epoch:  26  loss:  0.24323853850364685\n",
      "epoch:  27  loss:  0.2429238259792328\n",
      "epoch:  28  loss:  0.24260981380939484\n",
      "epoch:  29  loss:  0.242296501994133\n",
      "epoch:  30  loss:  0.24198386073112488\n",
      "epoch:  31  loss:  0.2416720688343048\n",
      "epoch:  32  loss:  0.24136199057102203\n",
      "epoch:  33  loss:  0.24105139076709747\n",
      "epoch:  34  loss:  0.24074147641658783\n",
      "epoch:  35  loss:  0.2404322326183319\n",
      "epoch:  36  loss:  0.24012364447116852\n",
      "epoch:  37  loss:  0.23981572687625885\n",
      "epoch:  38  loss:  0.2395085096359253\n",
      "epoch:  39  loss:  0.23920193314552307\n",
      "epoch:  40  loss:  0.23889602720737457\n",
      "epoch:  41  loss:  0.23859207332134247\n",
      "epoch:  42  loss:  0.238287553191185\n",
      "epoch:  43  loss:  0.2379836142063141\n",
      "epoch:  44  loss:  0.2376803308725357\n",
      "epoch:  45  loss:  0.23737767338752747\n",
      "epoch:  46  loss:  0.23707568645477295\n",
      "epoch:  47  loss:  0.23677434027194977\n",
      "epoch:  48  loss:  0.23647363483905792\n",
      "epoch:  49  loss:  0.23617354035377502\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "for epoch in range(50):\n",
    "   # Forward pass: Compute predicted y by passing x to the model\n",
    "   y_pred = model(x)\n",
    "\n",
    "   # Compute and print loss\n",
    "   loss = criterion(y_pred, y)\n",
    "   print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "   # Zero gradients, perform a backward pass, and update the weights.\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   # perform a backward pass (backpropagation)\n",
    "   loss.backward()\n",
    "\n",
    "   # Update the parameters\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # conv1's .weight\n",
    "\n",
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(input.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0678, -0.0346, -0.0489,  0.0146,  0.0358,  0.0892,  0.0144, -0.0107,\n",
      "         -0.0270,  0.0840]], grad_fn=<AddmmBackward>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tiamo\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:443: UserWarning: Using a target size (torch.Size([10])) that is different to the input size (torch.Size([1, 10])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  return F.mse_loss(input, target, reduction=self.reduction)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(38.2825, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.rand(1, 10), retain_graph=True)\n",
    "\n",
    "output = net(input)\n",
    "print(output)\n",
    "target = Variable(torch.Tensor([1,2,3,4,5,6,7,8,9,10]))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x0000020D38199128>\n",
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-df1981c54268>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv1.bias.grad before backward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'conv1.bias.grad after backward'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[0;32m    105\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[1;33m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m         \"\"\"\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[0;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[0;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 93\u001b[1;33m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[0;32m     94\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time."
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0678, -0.0346, -0.0489,  0.0146,  0.0358,  0.0892,  0.0144, -0.0107,\n",
      "         -0.0270,  0.0840]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad() # zero the gradient buffers\n",
    "output = net(input)\n",
    "print(output)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step() # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
