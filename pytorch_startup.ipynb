{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_in, n_h, n_out, batch_size = 10, 5, 1, 10\n",
    "x = torch.randn(batch_size, n_in)\n",
    "y = torch.tensor([[1.0], [0.0], [0.0], \n",
    "[1.0], [1.0], [1.0], [0.0], [0.0], [1.0], [1.0]])\n",
    "# Create a model\n",
    "model = nn.Sequential(nn.Linear(n_in, n_h),\n",
    "   nn.ReLU(),\n",
    "   nn.Linear(n_h, n_out),\n",
    "   nn.Sigmoid())\n",
    "# Construct the loss function\n",
    "criterion = torch.nn.MSELoss()\n",
    "# Construct the optimizer (Stochastic Gradient Descent in this case)\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:  0  loss:  0.2524194121360779\n",
      "epoch:  1  loss:  0.25228434801101685\n",
      "epoch:  2  loss:  0.25214970111846924\n",
      "epoch:  3  loss:  0.25201550126075745\n",
      "epoch:  4  loss:  0.2518816590309143\n",
      "epoch:  5  loss:  0.2517482340335846\n",
      "epoch:  6  loss:  0.2516152262687683\n",
      "epoch:  7  loss:  0.25148260593414307\n",
      "epoch:  8  loss:  0.25135037302970886\n",
      "epoch:  9  loss:  0.2512185275554657\n",
      "epoch:  10  loss:  0.25108709931373596\n",
      "epoch:  11  loss:  0.2509560286998749\n",
      "epoch:  12  loss:  0.25082531571388245\n",
      "epoch:  13  loss:  0.25069499015808105\n",
      "epoch:  14  loss:  0.2505650818347931\n",
      "epoch:  15  loss:  0.2504355013370514\n",
      "epoch:  16  loss:  0.25030630826950073\n",
      "epoch:  17  loss:  0.25017744302749634\n",
      "epoch:  18  loss:  0.250048965215683\n",
      "epoch:  19  loss:  0.24992084503173828\n",
      "epoch:  20  loss:  0.24979308247566223\n",
      "epoch:  21  loss:  0.24966567754745483\n",
      "epoch:  22  loss:  0.2495386153459549\n",
      "epoch:  23  loss:  0.24941186606884003\n",
      "epoch:  24  loss:  0.2492854744195938\n",
      "epoch:  25  loss:  0.24915944039821625\n",
      "epoch:  26  loss:  0.24903373420238495\n",
      "epoch:  27  loss:  0.24890834093093872\n",
      "epoch:  28  loss:  0.24878329038619995\n",
      "epoch:  29  loss:  0.24865856766700745\n",
      "epoch:  30  loss:  0.2485341727733612\n",
      "epoch:  31  loss:  0.24841009080410004\n",
      "epoch:  32  loss:  0.24828629195690155\n",
      "epoch:  33  loss:  0.2481628656387329\n",
      "epoch:  34  loss:  0.24803973734378815\n",
      "epoch:  35  loss:  0.24791689217090607\n",
      "epoch:  36  loss:  0.24779437482357025\n",
      "epoch:  37  loss:  0.2476721554994583\n",
      "epoch:  38  loss:  0.24755020439624786\n",
      "epoch:  39  loss:  0.24742859601974487\n",
      "epoch:  40  loss:  0.24730727076530457\n",
      "epoch:  41  loss:  0.24718622863292694\n",
      "epoch:  42  loss:  0.2470654845237732\n",
      "epoch:  43  loss:  0.24694500863552094\n",
      "epoch:  44  loss:  0.24682484567165375\n",
      "epoch:  45  loss:  0.24670493602752686\n",
      "epoch:  46  loss:  0.24658530950546265\n",
      "epoch:  47  loss:  0.24646598100662231\n",
      "epoch:  48  loss:  0.24634689092636108\n",
      "epoch:  49  loss:  0.24622808396816254\n"
     ]
    }
   ],
   "source": [
    "# Gradient Descent\n",
    "for epoch in range(50):\n",
    "   # Forward pass: Compute predicted y by passing x to the model\n",
    "   y_pred = model(x)\n",
    "\n",
    "   # Compute and print loss\n",
    "   loss = criterion(y_pred, y)\n",
    "   print('epoch: ', epoch,' loss: ', loss.item())\n",
    "\n",
    "   # Zero gradients, perform a backward pass, and update the weights.\n",
    "   optimizer.zero_grad()\n",
    "\n",
    "   # perform a backward pass (backpropagation)\n",
    "   loss.backward()\n",
    "\n",
    "   # Update the parameters\n",
    "   optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
       "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
       "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5) # 1 input image channel, 6 output channels, 5x5 square convolution kernel\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1   = nn.Linear(16*5*5, 120) # an affine operation: y = Wx + b\n",
    "        self.fc2   = nn.Linear(120, 84)\n",
    "        self.fc3   = nn.Linear(84, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2)) # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2) # If the size is a square you can only specify a single number\n",
    "        x = x.view(-1, self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:] # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "net = Net()\n",
    "net\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n",
      "torch.Size([1, 1, 32, 32])\n",
      "torch.Size([1, 10])\n"
     ]
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size()) # conv1's .weight\n",
    "\n",
    "input = Variable(torch.randn(1, 1, 32, 32))\n",
    "out = net(input)\n",
    "print(input.shape)\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0797, -0.0086,  0.1087,  0.0398, -0.0712, -0.1737, -0.0143, -0.1063,\n",
      "         -0.1455, -0.0326]], grad_fn=<AddmmBackward>)\n",
      "tensor(39.2281, grad_fn=<MseLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "net.zero_grad()\n",
    "out.backward(torch.rand(1, 10), retain_graph=True)\n",
    "\n",
    "output = net(input)\n",
    "print(output)\n",
    "target = Variable(torch.Tensor([1,2,3,4,5,6,7,8,9,10]))  # a dummy target, for example\n",
    "criterion = nn.MSELoss()\n",
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MseLossBackward object at 0x000001F7D7FD6F98>\n",
      "conv1.bias.grad before backward\n",
      "tensor([0., 0., 0., 0., 0., 0.])\n",
      "conv1.bias.grad after backward\n",
      "tensor([ 0.0609,  0.1111, -0.0187,  0.1179,  0.1608, -0.1728])\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)\n",
    "net.zero_grad()\n",
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "loss.backward()\n",
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1111, -0.0172,  0.1096,  0.0920, -0.0292, -0.0740,  0.0320, -0.0200,\n",
      "         -0.0838,  0.0637]], grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "# create your optimizer\n",
    "optimizer = optim.SGD(net.parameters(), lr = 0.01)\n",
    "\n",
    "# in your training loop:\n",
    "optimizer.zero_grad() # zero the gradient buffers\n",
    "output = net(input)\n",
    "print(output)\n",
    "loss = criterion(output, target)\n",
    "\n",
    "loss.backward()\n",
    "optimizer.step() # Does the update"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
